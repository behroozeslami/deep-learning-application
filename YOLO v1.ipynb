{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aa970bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as Tr\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a38799d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, cin, parameters, activation=nn.LeakyReLU(0.1)):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        cout        = parameters['cout']\n",
    "        kernel_size = parameters['kernel_size']\n",
    "        padding     = parameters['padding']\n",
    "        stride      = parameters['stride'] \n",
    "        \n",
    "        conv = nn.Conv2d(in_channels=cin, \n",
    "                         out_channels=cout, \n",
    "                         kernel_size=kernel_size, \n",
    "                         padding=padding,\n",
    "                         stride=stride,\n",
    "                         bias=False)\n",
    "        bn =  nn.BatchNorm2d(cout)\n",
    "        self.layer = nn.Sequential(*[conv, bn, activation])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.layer(x)\n",
    "    \n",
    "class FCLayers(nn.Module):\n",
    "    \n",
    "    def __init__(self, cin, fc_architecture, activation=nn.LeakyReLU(0.1)):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        S = fc_architecture['S']\n",
    "        B = fc_architecture['B']\n",
    "        C = fc_architecture['C']\n",
    "        c_hidden = fc_architecture['c_hidden']\n",
    "        \n",
    "        layer_list = []\n",
    "        layer_list.append(nn.Flatten())\n",
    "        layer_list.append(nn.Linear(cin*S*S, c_hidden))\n",
    "        layer_list.append(activation)\n",
    "        layer_list.append(nn.Linear(c_hidden, (C+5*B)*S*S))\n",
    "        self.layers = nn.Sequential(*layer_list)\n",
    "        self.S = S\n",
    "        self.C = C\n",
    "        self.B = B\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.layers(x).reshape(-1, self.S, self.S, self.C+5*self.B)\n",
    "\n",
    "class YOLO1(nn.Module):\n",
    "    \n",
    "    def __init__(self, cnn_architecture, fc_architecture, cin=3, activation=nn.LeakyReLU(0.1)):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        layer_list = []\n",
    "        c_input = cin\n",
    "        \n",
    "        for element in cnn_architecture:\n",
    "            if element['type']=='cnn_block':\n",
    "                for n in range(element['repeat']):\n",
    "                    for parameters in element['layers']:\n",
    "                        layer_list.append(ConvLayer(c_input, parameters, activation))\n",
    "                        c_input = parameters['cout']\n",
    "            if element['type']=='maxpool':\n",
    "                pool = nn.MaxPool2d(kernel_size=element['parameters']['kernel_size'], stride=element['parameters']['stride'])\n",
    "                layer_list.append(pool)\n",
    "                    \n",
    "        self.layers = nn.Sequential(*layer_list)\n",
    "        \n",
    "        self.fc_layers = FCLayers(c_input, fc_architecture, activation=activation)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.layers(x)\n",
    "        out = self.fc_layers(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b86b425",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, S=7, B=2, C=20):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        \n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "\n",
    "        # These are from Yolo paper, signifying how much we should\n",
    "        # pay loss for no object (noobj) and the box coordinates (coord)\n",
    "        self.lambda_noobj = 0.5\n",
    "        self.lambda_coord = 5\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        \n",
    "        class_probs = predictions[:,:,:,0:self.C]\n",
    "        Probs = predictions[:,:,:,self.C::5]\n",
    "        allbox_x = predictions[:,:,:,self.C+1::5]\n",
    "        allbox_y = predictions[:,:,:,self.C+2::5]\n",
    "        allbox_w = predictions[:,:,:,self.C+3::5]\n",
    "        allbox_h = predictions[:,:,:,self.C+4::5]\n",
    "        allbox_w_sqrt = torch.sqrt(torch.abs(allbox_w))*torch.sign(allbox_w)\n",
    "        allbox_h_sqrt = torch.sqrt(torch.abs(allbox_h))*torch.sign(allbox_h)\n",
    "\n",
    "        targetclass = target[:,:,:,0:self.C]\n",
    "        exists_box = target[:,:,:,[self.C]]\n",
    "        targetbox_x = target[:,:,:,[self.C+1]]\n",
    "        targetbox_y = target[:,:,:,[self.C+2]]\n",
    "        targetbox_w = target[:,:,:,[self.C+3]]\n",
    "        targetbox_h = target[:,:,:,[self.C+4]]\n",
    "        targetbox_w_sqrt = torch.sqrt(targetbox_w)\n",
    "        targetbox_h_sqrt = torch.sqrt(targetbox_h)\n",
    "\n",
    "        ### IOU Score ###\n",
    "        allbox_x1 = allbox_x - 0.5*allbox_w\n",
    "        allbox_x2 = allbox_x + 0.5*allbox_w\n",
    "        allbox_y1 = allbox_y - 0.5*allbox_h\n",
    "        allbox_y2 = allbox_y + 0.5*allbox_h\n",
    "\n",
    "        targetbox_x1 = targetbox_x - 0.5*targetbox_w\n",
    "        targetbox_x2 = targetbox_x + 0.5*targetbox_w\n",
    "        targetbox_y1 = targetbox_y - 0.5*targetbox_h\n",
    "        targetbox_y2 = targetbox_y + 0.5*targetbox_h\n",
    "\n",
    "        intersetbox_x1= torch.maximum(allbox_x1, targetbox_x1)\n",
    "        intersetbox_x2= torch.minimum(allbox_x2, targetbox_x2)\n",
    "        intersetbox_y1= torch.maximum(allbox_y1, targetbox_y1)\n",
    "        intersetbox_y2= torch.minimum(allbox_y2, targetbox_y2)\n",
    "\n",
    "        intersection_area = (intersetbox_x2-intersetbox_x1).clamp(0)*(intersetbox_y2 - intersetbox_y1).clamp(0)\n",
    "        allbox_area = torch.abs(allbox_w*allbox_h)\n",
    "        targetbox_area = torch.abs(targetbox_w*targetbox_h)\n",
    "        all_iou = intersection_area/(allbox_area+targetbox_area-intersection_area+ 1e-6)\n",
    "        maxiou_idx = all_iou.argmax(dim=-1)\n",
    "        maxiou_onehot = nn.functional.one_hot(maxiou_idx) # selects the box with higher iou\n",
    "\n",
    "        # ======================== #\n",
    "        #   FOR BOX COORDINATES    #\n",
    "        # ======================== #\n",
    "\n",
    "        box_loss = self.mse(maxiou_onehot*exists_box*targetbox_x, maxiou_onehot*exists_box*allbox_x)\n",
    "        box_loss += self.mse(maxiou_onehot*exists_box*targetbox_y, maxiou_onehot*exists_box*allbox_y)\n",
    "        box_loss += self.mse(maxiou_onehot*exists_box*targetbox_w_sqrt, maxiou_onehot*exists_box*allbox_w_sqrt)\n",
    "        box_loss += self.mse(maxiou_onehot*exists_box*targetbox_h_sqrt, maxiou_onehot*exists_box*allbox_h_sqrt)\n",
    "\n",
    "        # ======================= #\n",
    "        #   FOR OBJECT LOSS       #\n",
    "        # ======================= #\n",
    "\n",
    "        object_loss = self.mse(maxiou_onehot*exists_box, maxiou_onehot*exists_box*Probs)\n",
    "\n",
    "        # ================== #\n",
    "        #   FOR CLASS LOSS   #\n",
    "        # ================== #\n",
    "\n",
    "        class_loss = self.mse(exists_box*targetclass, exists_box*class_probs)\n",
    "\n",
    "        # ======================= #\n",
    "        #   FOR NO OBJECT LOSS    #\n",
    "        # ======================= #\n",
    "\n",
    "        no_object_loss = self.mse((1-exists_box)*exists_box, (1-exists_box)*Probs[:,:,:,[0]])\n",
    "        for j in range(1,self.B):\n",
    "            no_object_loss += self.mse((1-exists_box)*exists_box, (1-exists_box)*Probs[:,:,:,[j]])\n",
    "\n",
    "        # ======================= #\n",
    "        #   Total LOSS            #\n",
    "        # ======================= #   \n",
    "\n",
    "        loss = (\n",
    "                self.lambda_coord * box_loss  # first two rows in paper\n",
    "                + object_loss  # third row in paper\n",
    "                + self.lambda_noobj * no_object_loss  # forth row\n",
    "                + class_loss  # fifth row\n",
    "                )\n",
    "        \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89b0e796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n",
    "    \"\"\"\n",
    "    Calculates intersection over union\n",
    "\n",
    "    Parameters:\n",
    "        boxes_preds (tensor): Predictions of Bounding Boxes (BATCH_SIZE, 4)\n",
    "        boxes_labels (tensor): Correct labels of Bounding Boxes (BATCH_SIZE, 4)\n",
    "        box_format (str): midpoint/corners, if boxes (x,y,w,h) or (x1,y1,x2,y2)\n",
    "\n",
    "    Returns:\n",
    "        tensor: Intersection over union for all examples\n",
    "    \"\"\"\n",
    "\n",
    "    if box_format == \"midpoint\":\n",
    "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
    "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
    "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
    "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
    "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
    "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
    "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
    "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
    "\n",
    "    if box_format == \"corners\":\n",
    "        box1_x1 = boxes_preds[..., 0:1]\n",
    "        box1_y1 = boxes_preds[..., 1:2]\n",
    "        box1_x2 = boxes_preds[..., 2:3]\n",
    "        box1_y2 = boxes_preds[..., 3:4]  # (N, 1)\n",
    "        box2_x1 = boxes_labels[..., 0:1]\n",
    "        box2_y1 = boxes_labels[..., 1:2]\n",
    "        box2_x2 = boxes_labels[..., 2:3]\n",
    "        box2_y2 = boxes_labels[..., 3:4]\n",
    "\n",
    "    x1 = torch.max(box1_x1, box2_x1)\n",
    "    y1 = torch.max(box1_y1, box2_y1)\n",
    "    x2 = torch.min(box1_x2, box2_x2)\n",
    "    y2 = torch.min(box1_y2, box2_y2)\n",
    "\n",
    "    # .clamp(0) is for the case when they do not intersect\n",
    "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "\n",
    "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
    "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
    "\n",
    "    return intersection / (box1_area + box2_area - intersection + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "432f0d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, path, file, S=7, C=20, imagesize=448):\n",
    "        \n",
    "        self.data = pd.read_csv(os.path.join(path, file), header=None).values\n",
    "        self.sample_path = os.path.join(os.path.join(path, 'images'))\n",
    "        self.label_path = os.path.join(os.path.join(path, 'labels'))\n",
    "        self.S = S\n",
    "        self.C = C\n",
    "        self.imagesize = imagesize\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        \n",
    "        img = Image.open(os.path.join(self.sample_path, self.data[index,0]))\n",
    "        img = img.resize((self.imagesize,self.imagesize))\n",
    "        sample = np.asanyarray(img)\n",
    "        sample = sample/255\n",
    "        sample = torch.tensor(np.transpose(sample, (2,0,1)), dtype=torch.float)\n",
    "        label_raw = np.loadtxt(os.path.join(self.label_path, self.data[index,1]))\n",
    "        if len(label_raw.shape)==1:\n",
    "            label_raw = label_raw.reshape(1,5)\n",
    "        label = torch.zeros((self.S, self.S, self.C + 5))\n",
    "\n",
    "        for n in range(len(label_raw)):\n",
    "            c = int(label_raw[n,0])\n",
    "            loc = (self.S*label_raw[n,1:3]).astype(int)\n",
    "            box_center = self.S*label_raw[n,1:3] - loc\n",
    "            box_size = self.S*label_raw[n,3:5]\n",
    "            box = torch.tensor(np.concatenate((box_center, box_size)), dtype=torch.float)\n",
    "            label[loc[1], loc[0], c] = 1\n",
    "            label[loc[1], loc[0], self.C] = 1\n",
    "            label[loc[1], loc[0], self.C+1:] = box\n",
    "\n",
    "        \n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94690cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = './data/PascalVOC_YOLO/'\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_dataset = VOCDataset(path_data, 'train.csv', imagesize=448)\n",
    "\n",
    "test_dataset = VOCDataset(path_data, 'test.csv', imagesize=448)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f63eb2",
   "metadata": {},
   "source": [
    "-[source #1](https://arxiv.org/abs/1506.02640)\n",
    "\n",
    "-[source #2](https://www.youtube.com/watch?v=n9_XyCGr-MI&list=PLy5rjn5-uSPAKe2PfszYRqNY7JJC45P1d&index=7&t=1920s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6b80406",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_architecture = []\n",
    "\n",
    "element_1 = {'type': 'cnn_block',\n",
    "             'layers':[{'cout':64,\n",
    "                        'kernel_size':7,\n",
    "                        'padding': 3,\n",
    "                        'stride':2}],\n",
    "            'repeat': 1}\n",
    "\n",
    "cnn_architecture.append(element_1)\n",
    "\n",
    "element_2 = {'type': 'maxpool',\n",
    "             'parameters':{'kernel_size':2,\n",
    "                           'stride': 2}} \n",
    "\n",
    "cnn_architecture.append(element_2)\n",
    "\n",
    "element_3 = {'type': 'cnn_block',\n",
    "             'layers':[{'cout':192,\n",
    "                        'kernel_size':3,\n",
    "                        'padding': 1,\n",
    "                        'stride':1}],\n",
    "            'repeat': 1}\n",
    "\n",
    "cnn_architecture.append(element_3)\n",
    "\n",
    "element_4 = {'type': 'maxpool',\n",
    "             'parameters':{'kernel_size':2,\n",
    "                           'stride': 2}} \n",
    "\n",
    "cnn_architecture.append(element_4)\n",
    "\n",
    "element_5 = {'type': 'cnn_block',\n",
    "             'layers':[{'cout':128,\n",
    "                        'kernel_size':1,\n",
    "                        'padding': 0,\n",
    "                        'stride':1},\n",
    "                      {'cout':256,\n",
    "                        'kernel_size':3,\n",
    "                        'padding': 1,\n",
    "                        'stride':1},\n",
    "                      {'cout':256,\n",
    "                        'kernel_size':1,\n",
    "                        'padding': 0,\n",
    "                        'stride':1},\n",
    "                      {'cout':512,\n",
    "                        'kernel_size':3,\n",
    "                        'padding': 1,\n",
    "                        'stride':1}],\n",
    "            'repeat': 1}\n",
    "\n",
    "cnn_architecture.append(element_5)\n",
    "\n",
    "element_6 = {'type': 'maxpool',\n",
    "             'parameters':{'kernel_size':2,\n",
    "                           'stride': 2}} \n",
    "\n",
    "cnn_architecture.append(element_6)\n",
    "\n",
    "element_7 = {'type': 'cnn_block',\n",
    "             'layers':[{'cout':256,\n",
    "                        'kernel_size':1,\n",
    "                        'padding': 0,\n",
    "                        'stride':1},\n",
    "                      {'cout':512,\n",
    "                        'kernel_size':3,\n",
    "                        'padding': 1,\n",
    "                        'stride':1}],\n",
    "            'repeat': 4}\n",
    "\n",
    "cnn_architecture.append(element_7)\n",
    "\n",
    "element_8 = {'type': 'cnn_block',\n",
    "             'layers':[{'cout':512,\n",
    "                        'kernel_size':1,\n",
    "                        'padding': 0,\n",
    "                        'stride':1},\n",
    "                      {'cout':1024,\n",
    "                        'kernel_size':3,\n",
    "                        'padding': 1,\n",
    "                        'stride':1}],\n",
    "            'repeat': 1}\n",
    "\n",
    "cnn_architecture.append(element_8)\n",
    "\n",
    "element_9 = {'type': 'maxpool',\n",
    "             'parameters':{'kernel_size':2,\n",
    "                           'stride': 2}} \n",
    "\n",
    "cnn_architecture.append(element_9)\n",
    "\n",
    "element_10 = {'type': 'cnn_block',\n",
    "             'layers':[{'cout':512,\n",
    "                        'kernel_size':1,\n",
    "                        'padding': 0,\n",
    "                        'stride':1},\n",
    "                      {'cout':1024,\n",
    "                        'kernel_size':3,\n",
    "                        'padding': 1,\n",
    "                        'stride':1}],\n",
    "            'repeat': 2}\n",
    "\n",
    "cnn_architecture.append(element_10)\n",
    "\n",
    "element_11 = {'type': 'cnn_block',\n",
    "             'layers':[{'cout':1024,\n",
    "                        'kernel_size':3,\n",
    "                        'padding': 1,\n",
    "                        'stride':1},\n",
    "                      {'cout':1024,\n",
    "                        'kernel_size':3,\n",
    "                        'padding': 1,\n",
    "                        'stride':2},\n",
    "                      {'cout':1024,\n",
    "                        'kernel_size':3,\n",
    "                        'padding': 1,\n",
    "                        'stride':1},\n",
    "                      {'cout':1024,\n",
    "                        'kernel_size':3,\n",
    "                        'padding': 1,\n",
    "                        'stride':1}],\n",
    "            'repeat': 1}\n",
    "\n",
    "cnn_architecture.append(element_11)\n",
    "\n",
    "fc_architecture ={'S':7, 'B':2, 'C':20, 'c_hidden': 496}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c208a5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4629bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO1(cnn_architecture, fc_architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "111bd2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (x,y) in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "275598e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 448, 448])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f07f35b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 7, 7, 25])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c415505b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92754f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 7, 7, 30])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d43429d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = y\n",
    "predictions = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac0a543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ee410eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloLoss_v0(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculate the loss for yolo (v1) model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, S=7, B=2, C=20):\n",
    "        super(YoloLoss_v0, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "        \"\"\"\n",
    "        S is split size of image (in paper 7),\n",
    "        B is number of boxes (in paper 2),\n",
    "        C is number of classes (in paper and VOC dataset is 20),\n",
    "        \"\"\"\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "\n",
    "        # These are from Yolo paper, signifying how much we should\n",
    "        # pay loss for no object (noobj) and the box coordinates (coord)\n",
    "        self.lambda_noobj = 0.5\n",
    "        self.lambda_coord = 5\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "\n",
    "        # Calculate IoU for the two predicted bounding boxes with target bbox\n",
    "        iou_b1 = intersection_over_union(predictions[..., 21:25], target[..., 21:25])\n",
    "        iou_b2 = intersection_over_union(predictions[..., 26:30], target[..., 21:25])\n",
    "        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n",
    "\n",
    "        # Take the box with highest IoU out of the two prediction\n",
    "        # Note that bestbox will be indices of 0, 1 for which bbox was best\n",
    "        iou_maxes, bestbox = torch.max(ious, dim=0)\n",
    "        exists_box = target[..., 20].unsqueeze(3)  # in paper this is Iobj_i\n",
    "\n",
    "        # ======================== #\n",
    "        #   FOR BOX COORDINATES    #\n",
    "        # ======================== #\n",
    "\n",
    "        # Set boxes with no object in them to 0. We only take out one of the two \n",
    "        # predictions, which is the one with highest Iou calculated previously.\n",
    "        box_predictions = exists_box * (\n",
    "            (\n",
    "                bestbox * predictions[..., 26:30]\n",
    "                + (1 - bestbox) * predictions[..., 21:25]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        box_targets = exists_box * target[..., 21:25]\n",
    "\n",
    "        # Take sqrt of width, height of boxes to ensure that\n",
    "        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n",
    "            torch.abs(box_predictions[..., 2:4] + 1e-6)\n",
    "        )\n",
    "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
    "\n",
    "        box_loss = self.mse(\n",
    "            torch.flatten(box_predictions, end_dim=-2),\n",
    "            torch.flatten(box_targets, end_dim=-2),\n",
    "        )\n",
    "\n",
    "        # ==================== #\n",
    "        #   FOR OBJECT LOSS    #\n",
    "        # ==================== #\n",
    "\n",
    "        # pred_box is the confidence score for the bbox with highest IoU\n",
    "        pred_box = (\n",
    "            bestbox * predictions[..., 25:26] + (1 - bestbox) * predictions[..., 20:21]\n",
    "        )\n",
    "\n",
    "        object_loss = self.mse(\n",
    "            torch.flatten(exists_box * pred_box),\n",
    "            torch.flatten(exists_box * target[..., 20:21]),\n",
    "        )\n",
    "\n",
    "        # ======================= #\n",
    "        #   FOR NO OBJECT LOSS    #\n",
    "        # ======================= #\n",
    "\n",
    "        #max_no_obj = torch.max(predictions[..., 20:21], predictions[..., 25:26])\n",
    "        #no_object_loss = self.mse(\n",
    "        #    torch.flatten((1 - exists_box) * max_no_obj, start_dim=1),\n",
    "        #    torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1),\n",
    "        #)\n",
    "\n",
    "        no_object_loss = self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., 20:21], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1),\n",
    "        )\n",
    "\n",
    "        no_object_loss += self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., 25:26], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1)\n",
    "        )\n",
    "\n",
    "        # ================== #\n",
    "        #   FOR CLASS LOSS   #\n",
    "        # ================== #\n",
    "\n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(exists_box * predictions[..., :20], end_dim=-2,),\n",
    "            torch.flatten(exists_box * target[..., :20], end_dim=-2,),\n",
    "        )\n",
    "\n",
    "        loss = (\n",
    "            self.lambda_coord * box_loss  # first two rows in paper\n",
    "            + object_loss  # third row in paper\n",
    "            + self.lambda_noobj * no_object_loss  # forth row\n",
    "            + class_loss  # fifth row\n",
    "        )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e1f2c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1247.9930, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossfn_0 = YoloLoss_v0()\n",
    "lossfn_0(predictions, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef279fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b80706f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1247.9955, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossfn = YoloLoss()\n",
    "lossfn(predictions, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56fc3f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf98abc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
